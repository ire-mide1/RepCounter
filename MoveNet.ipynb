{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9b14e9-7600-4041-b017-ed0dda1c1333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iremide\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\Iremide\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ff8b45-2606-4143-8d1a-ec7f008b4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture object from the webcam (index 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Capture frame-by-frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Exit the loop if the frame was not captured successfully\n",
    "\n",
    "    image = frame.copy()  # Create a copy of the captured frame\n",
    "    height, width, _ = frame.shape  # Get the height and width of the frame\n",
    "\n",
    "    # Convert the captured frame to a Tensor and resize it to 192x192 with padding\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.image.resize_with_pad(input_image, 192, 192)\n",
    "\n",
    "    # Load the pre-trained MoveNet model for pose estimation\n",
    "    model_path = \"movenet_lightning_fp16.tflite\"\n",
    "    interpreter = tf.lite.Interpreter(model_path)\n",
    "    interpreter.allocate_tensors()  # Allocate memory for the model's tensors\n",
    "\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)  # Cast the image to uint8 type\n",
    "    input_details = interpreter.get_input_details()  # Get model input details\n",
    "    output_details = interpreter.get_output_details()  # Get model output details\n",
    "\n",
    "    # Set the input tensor and run inference\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    interpreter.invoke()  # Perform inference\n",
    "    keypoints = interpreter.get_tensor(output_details[0]['index'])  # Get the detected keypoints\n",
    "\n",
    "    # Define the edges between keypoints for drawing\n",
    "    KEYPOINT_EDGES = [(0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 7),\n",
    "                      (7, 9), (6, 8), (8, 10), (5, 6), (5, 11), (6, 12), \n",
    "                      (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)]\n",
    "\n",
    "    # Draw keypoints on the frame\n",
    "    for keypoint in keypoints[0][0]:\n",
    "        y, x, confidence = keypoint  # Extract y, x coordinates and confidence score\n",
    "        if confidence > 0.3:  # Check if the confidence score is above the threshold\n",
    "            cx = int(x * width)  # Convert normalized x-coordinate to pixel value\n",
    "            cy = int(y * height)  # Convert normalized y-coordinate to pixel value\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)  # Draw keypoint as a red circle\n",
    "\n",
    "    # Draw edges between keypoints\n",
    "    for edge in KEYPOINT_EDGES:\n",
    "        p1 = keypoints[0][0][edge[0]]  # Get the first keypoint in the edge\n",
    "        p2 = keypoints[0][0][edge[1]]  # Get the second keypoint in the edge\n",
    "\n",
    "        y1, x1, c1 = p1  # Extract y, x coordinates and confidence score of the first keypoint\n",
    "        y2, x2, c2 = p2  # Extract y, x coordinates and confidence score of the second keypoint\n",
    "\n",
    "        if c1 > 0.3 and c2 > 0.3:  # Only draw edges if confidence for both keypoints is high\n",
    "            pt1 = (int(x1 * width), int(y1 * height))  # Convert first keypoint to pixel coordinates\n",
    "            pt2 = (int(x2 * width), int(y2 * height))  # Convert second keypoint to pixel coordinates\n",
    "            cv2.line(frame, pt1, pt2, (0, 255, 0), 2)  # Draw edge as a green line\n",
    "\n",
    "    # Display the frame with drawn keypoints and edges\n",
    "    cv2.imshow('MoveNet Lightning', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da21d1c7-0833-422b-8a73-b25b374a7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import atan2, degrees\n",
    "\n",
    "# Function to calculate angle between three points\n",
    "def calculate_angle(p1, p2, p3):\n",
    "    a = np.array(p1)\n",
    "    b = np.array(p2)\n",
    "    c = np.array(p3)\n",
    "    \n",
    "    ab = a - b\n",
    "    bc = c - b\n",
    "    \n",
    "    angle = atan2(np.linalg.det([ab, bc]), np.dot(ab, bc))\n",
    "    return abs(degrees(angle))\n",
    "\n",
    "# Initialize variables for curl counting\n",
    "left_curls_count = 0\n",
    "right_curls_count = 0\n",
    "left_stage = \"down\"\n",
    "right_stage = \"down\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image = frame.copy()\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Convert image to Tensor and resize with padding to the model's expected input size (192x192)\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.image.resize_with_pad(input_image, 192, 192)\n",
    "\n",
    "    model_path = \"movenet_lightning_fp16.tflite\"\n",
    "    interpreter = tf.lite.Interpreter(model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    interpreter.invoke()\n",
    "    keypoints = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    # Keypoint format: [y, x, score]\n",
    "    KEYPOINT_EDGES = [(0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 7),\n",
    "                      (7, 9), (6, 8), (8, 10), (5, 6), (5, 11), (6, 12), \n",
    "                      (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)]\n",
    "\n",
    "    # Draw keypoints and edges back on the original frame\n",
    "    for keypoint in keypoints[0][0]:\n",
    "        y, x, confidence = keypoint\n",
    "        if confidence > 0.3:  # Threshold for displaying keypoints\n",
    "            cx = int(x * width)\n",
    "            cy = int(y * height)\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "\n",
    "    # Draw edges between keypoints\n",
    "    for edge in KEYPOINT_EDGES:\n",
    "        p1 = keypoints[0][0][edge[0]]\n",
    "        p2 = keypoints[0][0][edge[1]]\n",
    "\n",
    "        y1, x1, c1 = p1\n",
    "        y2, x2, c2 = p2\n",
    "\n",
    "        if c1 > 0.3 and c2 > 0.3:  # Only draw edges if confidence is high\n",
    "            pt1 = (int(x1 * width), int(y1 * height))\n",
    "            pt2 = (int(x2 * width), int(y2 * height))\n",
    "            cv2.line(frame, pt1, pt2, (0, 255, 0), 2)\n",
    "\n",
    "    # Extract keypoints for left and right arms\n",
    "    left_shoulder = keypoints[0][0][5]\n",
    "    left_elbow = keypoints[0][0][7]\n",
    "    left_wrist = keypoints[0][0][9]\n",
    "\n",
    "    right_shoulder = keypoints[0][0][6]\n",
    "    right_elbow = keypoints[0][0][10]\n",
    "    right_wrist = keypoints[0][0][5]\n",
    "\n",
    "    # Calculate angles for left and right elbows\n",
    "    left_elbow_angle = calculate_angle(\n",
    "        (left_shoulder[1] * width, left_shoulder[0] * height),\n",
    "        (left_elbow[1] * width, left_elbow[0] * height),\n",
    "        (left_wrist[1] * width, left_wrist[0] * height)\n",
    "    )\n",
    "    \n",
    "    right_elbow_angle = calculate_angle(\n",
    "        (right_shoulder[1] * width, right_shoulder[0] * height),\n",
    "        (right_elbow[1] * width, right_elbow[0] * height),\n",
    "        (right_wrist[1] * width, right_wrist[0] * height)\n",
    "    )\n",
    "\n",
    "    # Left curl counting logic\n",
    "    if left_elbow_angle > 160:\n",
    "        left_stage = \"down\"\n",
    "    if left_elbow_angle < 30 and left_stage == \"down\":\n",
    "        left_curls_count += 1\n",
    "        left_stage = \"up\"\n",
    "\n",
    "    # Right curl counting logic\n",
    "    #if right_elbow_angle > 160:\n",
    "    #    right_stage = \"down\"\n",
    "    #if right_elbow_angle < 30 and right_stage == \"down\":\n",
    "    #    right_curls_count += 1\n",
    "    #    right_stage = \"up\"\n",
    "\n",
    "    # Show the frame with keypoints and edges\n",
    "    cv2.putText(frame, f\"Left Curls: {left_curls_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    #cv2.putText(frame, f\"Right Curls: {right_curls_count}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('MoveNet Lightning', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01e741-7ee6-4a6f-b624-3a0c9a84174d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
